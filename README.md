# Music-Generation-Using-RNN with LSTM
## Overview
This project explores music generation using deep learning, leveraging LSTM networks and autoencoders to predict and reconstruct MIDI note sequences. The LSTM model learns musical patterns and predicts the next noteâ€™s pitch, step (time gap), and duration, while an autoencoder compresses and reconstructs MIDI data efficiently. The model is trained using Sparse Categorical Crossentropy for pitch and MSE with positivity constraints for step and duration, ensuring effective learning with minimal loss. Generated sequences are analyzed through piano roll plots and loss function trends to evaluate performance. Additionally, a convolutional autoencoder reduces data dimensionality by encoding it into a 16-dimensional latent space for reconstruction. This project highlights the potential of AI in music composition, data compression, and creative applications by maintaining musical structure and coherence. ðŸš€ðŸŽµ

Dataset Used
https://magenta.tensorflow.org/datasets/maestro#v200
## Contributions
Contributions to this project are welcome. Feel free to open a pull request or an issue if you have any suggestions or bug reports.
## Members 
- [Shweta](https://github.com/me-shweta)
