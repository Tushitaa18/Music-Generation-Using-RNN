# -*- coding: utf-8 -*-
"""music_generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fBYku_Bg8443647LoEdSTYe0uYS2M2ka

# Music Generation using RNN

Dataset - Piano MIDI files - [MAESTRO dataset](https://magenta.tensorflow.org/datasets/maestro)

## Setup
"""

!pip install pyfluidsynth

!sudo apt install -y fluidsynth

!pip install --upgrade pyfluidsynth

!pip install pretty_midi

import collections
import datetime
import fluidsynth
import glob
import numpy as np
import pathlib
import pandas as pd
import pretty_midi
import seaborn as sns
import tensorflow as tf
from IPython import display
from matplotlib import pyplot as plt
from typing import Optional

import librosa
import librosa.display
import IPython.display as ipd
import glob
import os

seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)

# Sampling rate for audio playback
_SAMPLING_RATE = 16000

"""## Download the Maestro dataset"""

data_dir = pathlib.Path('data/maestro-v2.0.0')
if not data_dir.exists():
  tf.keras.utils.get_file(
      'maestro-v2.0.0-midi.zip',
      origin='https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip',
      extract=True,
      cache_dir='.', cache_subdir='data',
  )

filenames = glob.glob(str(data_dir/'**/*.mid*'))
print('Number of files:', len(filenames))

sequence_length = 100
network_input = []
network_output = []

"""## Process a MIDI file"""

sample_file = filenames[1]
print(sample_file)

pm = pretty_midi.PrettyMIDI(sample_file)
print(pm)

print("Tempo Changes:", pm.get_tempo_changes())
print("Time Signature Changes:", pm.time_signature_changes)
print("Key Signature Changes:", pm.key_signature_changes)

print("Instruments:", pm.instruments)  # Lists all instruments in the MIDI file
for i, instrument in enumerate(pm.instruments):
    print(f"\nInstrument {i+1}:")
    print("  Name:", instrument.name)
    print("  Program:", instrument.program)
    print("  Is Drum:", instrument.is_drum)
    print("  Number of Notes:", len(instrument.notes))
    print("  Number of Pitch Bends:", len(instrument.pitch_bends))
    print("  Number of Control Changes:", len(instrument.control_changes))

midi_files = glob.glob(str(data_dir/'**/*.mid*'))

midi_files = midi_files[:10]

file_data = []

for file_path in midi_files:
    file_name = os.path.basename(file_path)

    pm = pretty_midi.PrettyMIDI(file_path)

    tempo_changes = pm.get_tempo_changes()
    file_info = {
        "File Name": file_name,
        "Tempo Changes (Count)": len(tempo_changes[0]),
        "Time Signature Changes (Count)": len(pm.time_signature_changes),
        "Key Signature Changes (Count)": len(pm.key_signature_changes),
        "Number of Instruments": len(pm.instruments),
    }

    # Aggregate instrument data
    total_notes = sum(len(inst.notes) for inst in pm.instruments)
    total_pitch_bends = sum(len(inst.pitch_bends) for inst in pm.instruments)
    total_control_changes = sum(len(inst.control_changes) for inst in pm.instruments)

    file_info["Total Notes"] = total_notes
    file_info["Total Pitch Bends"] = total_pitch_bends
    file_info["Total Control Changes"] = total_control_changes

    # Append to the list
    file_data.append(file_info)

# Convert to DataFrame
df = pd.DataFrame(file_data)
print(df)

# Save DataFrame to CSV
df.to_csv("midi_file_summary.csv", index=False)

import matplotlib.pyplot as plt

# Adjust figure size and layout
plt.figure(figsize=(16, 8))

# Tempo Changes
plt.subplot(1, 3, 1)
plt.bar(df["File Name"], df["Tempo Changes (Count)"], color='skyblue')
plt.title("Tempo Changes")
plt.xticks(rotation=45, fontsize=8, ha='right')
plt.ylabel("Count")
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Total Notes
plt.subplot(1, 3, 2)
plt.bar(df["File Name"], df["Total Notes"], color='lightgreen')
plt.title("Total Notes")
plt.xticks(rotation=45, fontsize=8, ha='right')
plt.ylabel("Count")
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Number of Instruments
plt.subplot(1, 3, 3)
plt.bar(df["File Name"], df["Number of Instruments"], color='salmon')
plt.title("Number of Instruments")
plt.xticks(rotation=45, fontsize=8, ha='right')
plt.ylabel("Count")
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Use tight layout to optimize spacing
plt.tight_layout()
plt.show()

print("\nNotes in first instrument:")
for note in pm.instruments[0].notes[:5]:
    print(f"  Start: {note.start}, End: {note.end}, Pitch: {note.pitch}, Velocity: {note.velocity}")

def display_audio(pm: pretty_midi.PrettyMIDI, seconds=30):
  waveform = pm.fluidsynth(fs=_SAMPLING_RATE)
  waveform_short = waveform[:seconds*_SAMPLING_RATE]
  return display.Audio(waveform_short, rate=_SAMPLING_RATE)

display_audio(pm)

print('Number of instruments:', len(pm.instruments))
instrument = pm.instruments[0]
instrument_name = pretty_midi.program_to_instrument_name(instrument.program)
print('Instrument name:', instrument_name)

# List the first three files in your dataset
for i, file in enumerate(filenames[:3]):
    print(f"File {i + 1}: {file}")

import pretty_midi
import librosa.display
import IPython.display as ipd

# Function to convert MIDI to audio
def midi_to_audio(midi_file):
    midi_data = pretty_midi.PrettyMIDI(midi_file)
    audio_signal = midi_data.fluidsynth()  # Uses default soundfont to synthesize audio
    return audio_signal

# Example MIDI files
first_file = "data/maestro-v2.0.0/2009/MIDI-Unprocessed_12_R1_2009_01-02_ORIG_MID--AUDIO_12_R1_2009_12_R1_2009_01_WAV.midi"
second_file = "data/maestro-v2.0.0/2009/MIDI-Unprocessed_12_R1_2009_01-02_ORIG_MID--AUDIO_12_R1_2009_12_R1_2009_01_WAV.midi"
third_file = "data/maestro-v2.0.0/2009/MIDI-Unprocessed_03_R1_2009_03-08_ORIG_MID--AUDIO_03_R1_2009_03_R1_2009_04_WAV.midi"

# Convert MIDI to audio
first_audio = midi_to_audio(first_file)
second_audio = midi_to_audio(second_file)
third_audio = midi_to_audio(third_file)

# Play the first audio file
ipd.Audio(first_audio, rate=22050)

print(f"First Audio Length: {len(first_audio)} samples")
print(f"Second Audio Length: {len(second_audio)} samples")
print(f"Third Audio Length: {len(third_audio)} samples")

sr = 22050  # Assuming your MIDI to audio conversion used a 22050 Hz sample rate
first = first_audio
second = second_audio
third = third_audio

"""###Basic information regarding audio files"""

first.shape

# duration in seconds of 1 sample
sample_duration = 1 / sr
print(f"One sample lasts for {sample_duration:6f} seconds")

# total number of samples in audio file
tot_samples = len(first)
tot_samples

# duration of first audio in seconds
duration = 1 / sr * tot_samples
print(f"The audio lasts for {duration} seconds")

"""###Visualising audio signal in the time domain"""

plt.figure(figsize=(15, 17))

plt.subplot(3, 1, 1)
librosa.display.waveshow(first, alpha=0.5, sr=sr)
plt.ylim((-1, 1))
plt.title("first")

plt.subplot(3, 1, 2)
librosa.display.waveshow(second, alpha=0.5, sr=sr)
plt.ylim((-1, 1))
plt.title("second")

plt.subplot(3, 1, 3)
librosa.display.waveshow(second, alpha=0.5, sr=sr)
plt.ylim((-1, 1))
plt.title("third")

plt.show()

"""###Calculating amplitude envelope"""

FRAME_SIZE = 1024
HOP_LENGTH = 512

def amplitude_envelope(signal, frame_size, hop_length):
    """Calculate the amplitude envelope of a signal with a given frame size nad hop length."""
    amplitude_envelope = []

    # calculate amplitude envelope for each frame
    for i in range(0, len(signal), hop_length):
        amplitude_envelope_current_frame = max(signal[i:i+frame_size])
        amplitude_envelope.append(amplitude_envelope_current_frame)

    return np.array(amplitude_envelope)

def fancy_amplitude_envelope(signal, frame_size, hop_length):
    return np.array([max(signal[i:i+frame_size]) for i in range(0, len(signal), hop_length)])

# number of frames in amplitude envelope
ae_first = amplitude_envelope(first, FRAME_SIZE, HOP_LENGTH)
len(ae_first)

# calculate amplitude envelope for first and second
ae_second = amplitude_envelope(second, FRAME_SIZE, HOP_LENGTH)
ae_third = amplitude_envelope(third, FRAME_SIZE, HOP_LENGTH)

"""###Visualising amplitude envelope"""

frames = range(len(ae_first))
t = librosa.frames_to_time(frames, hop_length=HOP_LENGTH)

import librosa
import librosa.display
import matplotlib.pyplot as plt

# Assuming first, second, third are already loaded with librosa.load()
# Assuming ae_first, ae_second, ae_third are already computed

# Visualizing the amplitude envelope for each audio file
plt.figure(figsize=(15, 17))

# Plot for first audio
ax = plt.subplot(3, 1, 1)
librosa.display.waveshow(first, alpha=0.5)  # Using waveshow for visualization
t_first = librosa.frames_to_time(range(len(ae_first)), hop_length=HOP_LENGTH) # Generate time vector based on ae_first
plt.plot(t_first, ae_first, color="r")  # Plot amplitude envelope in red
plt.ylim((-1, 1))
plt.title("First")

# Plot for second audio
plt.subplot(3, 1, 2)
librosa.display.waveshow(second, alpha=0.5)  # Using waveshow for visualization
t_second = librosa.frames_to_time(range(len(ae_second)), hop_length=HOP_LENGTH) # Generate time vector based on ae_second
plt.plot(t_second, ae_second, color="r")  # Plot amplitude envelope in red
plt.ylim((-1, 1))
plt.title("Second")

# Plot for third audio
plt.subplot(3, 1, 3)
librosa.display.waveshow(third, alpha=0.5)  # Using waveshow for visualization
t_third = librosa.frames_to_time(range(len(ae_third)), hop_length=HOP_LENGTH) # Generate time vector based on ae_third
plt.plot(t_third, ae_third, color="r")  # Plot amplitude envelope in red
plt.ylim((-1, 1))
plt.title("Third")

# Show the plot
plt.show()

"""###Root-mean-squared energy with Librosa"""

FRAME_SIZE = 1024
HOP_LENGTH = 512

rms_first = librosa.feature.rms(y=first, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]
rms_second = librosa.feature.rms(y=second, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]
rms_third = librosa.feature.rms(y=third, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]

"""###Visualise RMSE + waveform"""

frames = range(len(rms_first))
t = librosa.frames_to_time(frames, hop_length=HOP_LENGTH)

!pip install librosa
import librosa
import librosa.display
import matplotlib.pyplot as plt

FRAME_SIZE = 1024
HOP_LENGTH = 512

# Assuming first, second, and third are your audio data

rms_first = librosa.feature.rms(y=first, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]
rms_second = librosa.feature.rms(y=second, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]
rms_third = librosa.feature.rms(y=third, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]

# Generate separate time axes for each audio segment
frames_first = range(len(rms_first))
t_first = librosa.frames_to_time(frames_first, hop_length=HOP_LENGTH)

frames_second = range(len(rms_second))
t_second = librosa.frames_to_time(frames_second, hop_length=HOP_LENGTH)

frames_third = range(len(rms_third))
t_third = librosa.frames_to_time(frames_third, hop_length=HOP_LENGTH)

# rms energy is graphed in red
plt.figure(figsize=(15, 17))

ax = plt.subplot(3, 1, 1)
librosa.display.waveshow(first, alpha=0.5)
# Use t_first for plotting rms_first
plt.plot(t_first, rms_first, color="r")
plt.ylim((-1, 1))
plt.title("First")

plt.subplot(3, 1, 2)
librosa.display.waveshow(second, alpha=0.5)
# Use t_second for plotting rms_second
plt.plot(t_second, rms_second, color="r")
plt.ylim((-1, 1))
plt.title("Second")

plt.subplot(3, 1, 3)
librosa.display.waveshow(third, alpha=0.5)
# Use t_third for plotting rms_third
plt.plot(t_third, rms_third, color="r")
plt.ylim((-1, 1))
plt.title("Third")

plt.show()

"""###RMSE from scratch"""

def rmse(signal, frame_size, hop_length):
    rmse = []

    # calculate rmse for each frame
    for i in range(0, len(signal), hop_length):
        rmse_current_frame = np.sqrt(sum(signal[i:i+frame_size]**2) / frame_size)
        rmse.append(rmse_current_frame)
    return np.array(rmse)

rms_first1 = rmse(first, FRAME_SIZE, HOP_LENGTH)
rms_second1 = rmse(second, FRAME_SIZE, HOP_LENGTH)
rms_third1 = rmse(third, FRAME_SIZE, HOP_LENGTH)

import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np # Importing numpy for rmse function

FRAME_SIZE = 1024
HOP_LENGTH = 512

# Assuming first, second, and third are your audio data, and sr is the sample rate

# ... (Your rmse function remains the same) ...

rms_first = librosa.feature.rms(y=first, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]
rms_second = librosa.feature.rms(y=second, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]
rms_third = librosa.feature.rms(y=third, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]

rms_first1 = rmse(first, FRAME_SIZE, HOP_LENGTH)
rms_second1 = rmse(second, FRAME_SIZE, HOP_LENGTH)
rms_third1 = rmse(third, FRAME_SIZE, HOP_LENGTH)

# Generate separate time axes for each audio segment BASED ON RMS LENGTH
frames_first = range(len(rms_first))
t_first = librosa.frames_to_time(frames_first, hop_length=HOP_LENGTH)

frames_second = range(len(rms_second))
t_second = librosa.frames_to_time(frames_second, hop_length=HOP_LENGTH)

frames_third = range(len(rms_third))
t_third = librosa.frames_to_time(frames_third, hop_length=HOP_LENGTH)

# ... (Your rmse function remains the same) ...


plt.figure(figsize=(15, 17))

ax = plt.subplot(3, 1, 1)
librosa.display.waveshow(first, sr=sr, alpha=0.5)
# Use t_first which is derived from rms_first length for plotting
plt.plot(t_first, rms_first, color="r")
plt.plot(t_first, rms_first1[:len(t_first)], color="y") # Making sure rms_first1 matches t_first length
plt.ylim((-1, 1))
plt.title("First")

plt.subplot(3, 1, 2)
librosa.display.waveshow(second, sr=sr, alpha=0.5)
# Use t_second which is derived from rms_second length for plotting
plt.plot(t_second, rms_second, color="r")
plt.plot(t_second, rms_second1[:len(t_second)], color="y") # Making sure rms_second1 matches t_second length
plt.ylim((-1, 1))
plt.title("Second")

plt.subplot(3, 1, 3)
librosa.display.waveshow(third, sr=sr, alpha=0.5)
# Use t_third which is derived from rms_third length for plotting
plt.plot(t_third, rms_third, color="r")
plt.plot(t_third, rms_third1[:len(t_third)], color="y")  # Making sure rms_third1 matches t_third length
plt.ylim((-1, 1))

"""## Extract notes"""

for i, note in enumerate(instrument.notes[:10]):
  note_name = pretty_midi.note_number_to_name(note.pitch)
  duration = note.end - note.start
  print(f'{i}: pitch={note.pitch}, note_name={note_name},'
        f' duration={duration:.4f}')

"""We will use three variables to represent a note when training the model: `pitch`, `step` and `duration`. The pitch is the perceptual quality of the sound as a MIDI note number.
The `step` is the time elapsed from the previous note or start of the track.
The `duration` is how long the note will be playing in seconds and is the difference between the note end and note start times.

"""

def midi_to_notes(midi_file: str) -> pd.DataFrame:
  pm = pretty_midi.PrettyMIDI(midi_file)
  instrument = pm.instruments[0]
  notes = collections.defaultdict(list)

  # Sort the notes by start time
  sorted_notes = sorted(instrument.notes, key=lambda note: note.start)
  prev_start = sorted_notes[0].start

  for note in sorted_notes:
    start = note.start
    end = note.end
    notes['pitch'].append(note.pitch)
    notes['start'].append(start)
    notes['end'].append(end)
    notes['step'].append(start - prev_start)
    notes['duration'].append(end - start)
    prev_start = start

  return pd.DataFrame({name: np.array(value) for name, value in notes.items()})

raw_notes = midi_to_notes(sample_file)
raw_notes.head()

def midi_to_notes(midi_file: str) -> pd.DataFrame:
    pm = pretty_midi.PrettyMIDI(midi_file)
    instrument = pm.instruments[0]
    notes = collections.defaultdict(list)

    # Sort the notes by start time
    sorted_notes = sorted(instrument.notes, key=lambda note: note.start)
    prev_start = sorted_notes[0].start

    # Loop through each note
    for note in sorted_notes:
        start = note.start
        end = note.end
        notes['pitch'].append(note.pitch)
        notes['start'].append(start)
        notes['end'].append(end)
        notes['step'].append(start - prev_start)
        notes['duration'].append(end - start)
        notes['filename'].append(midi_file)  # Add the filename to each row
        prev_start = start

    # Return as DataFrame
    return pd.DataFrame({name: np.array(value) for name, value in notes.items()})

raw_notes = midi_to_notes(sample_file)
raw_notes.head()

"""Converting numeric pitch to note names makes it easier to understand. The note name indicates the type of note, any accidentals, and the octave number (e.g., C#4)."""

get_note_names = np.vectorize(pretty_midi.note_number_to_name)
sample_note_names = get_note_names(raw_notes['pitch'])
sample_note_names[:10]

"""To visualize the musical piece, plot the note pitch, start and end across the length of the track (i.e. piano roll). Start with the first 100 notes"""

#creating a piano roll visualization
def plot_piano_roll(notes: pd.DataFrame, count: Optional[int] = None):
  if count:
    title = f'First {count} notes'
  else:
    title = f'Whole track'
    count = len(notes['pitch'])
  plt.figure(figsize=(20, 4))
  plot_pitch = np.stack([notes['pitch'], notes['pitch']], axis=0)
  plot_start_stop = np.stack([notes['start'], notes['end']], axis=0)
  plt.plot(
      plot_start_stop[:, :count], plot_pitch[:, :count], color="b", marker=".")
  plt.xlabel('Time [s]')
  plt.ylabel('Pitch')
  _ = plt.title(title)

plot_piano_roll(raw_notes, count=100)

def plot_enhanced_piano_roll(notes: pd.DataFrame, count: Optional[int] = None):
    """
    Function to create an enhanced piano roll visualization with gradient colors.

    Arguments:
    notes -- DataFrame containing note information (pitch, start, end, velocity).
    count -- Number of notes to display (optional).
    """
    if count:
        title = f'First {count} notes'
        count = min(count, len(notes))  # Limit count to available notes
    else:
        title = 'Whole track'
        count = len(notes)

    # Extract necessary data
    plot_pitch = np.stack([notes['pitch'], notes['pitch']], axis=0)[:, :count]
    plot_start_stop = np.stack([notes['start'], notes['end']], axis=0)[:, :count]
    velocities = notes.get('velocity', pd.Series([100] * len(notes)))[:count]  # Default velocity if not provided

    # Normalize velocities for color mapping
    norm = plt.Normalize(velocities.min(), velocities.max())
    cmap = plt.cm.Blues

    plt.figure(figsize=(20, 6))
    for i in range(count):
        plt.plot(
            plot_start_stop[:, i], plot_pitch[:, i],
            color=cmap(norm(velocities.iloc[i])),
            linewidth=2, marker="."
        )

    # Add labels and title
    plt.xlabel('Time [s]')
    plt.ylabel('Pitch')
    plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap), label="Velocity (Intensity)")
    plt.title(title)
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.show()

plot_piano_roll(raw_notes, count=50)

"""Notes for the entire track."""

plot_piano_roll(raw_notes)

"""Distribution of each note variable."""

def plot_distributions(notes: pd.DataFrame, drop_percentile=2.5):
  plt.figure(figsize=[15, 5])
  plt.subplot(1, 3, 1)
  sns.histplot(notes, x="pitch", bins=20)

  plt.subplot(1, 3, 2)
  max_step = np.percentile(notes['step'], 100 - drop_percentile)
  sns.histplot(notes, x="step", bins=np.linspace(0, max_step, 21))

  plt.subplot(1, 3, 3)
  max_duration = np.percentile(notes['duration'], 100 - drop_percentile)
  sns.histplot(notes, x="duration", bins=np.linspace(0, max_duration, 21))

plot_distributions(raw_notes)

#Box Plots
plt.figure(figsize=[15, 5])
plt.subplot(1, 3, 1)
sns.boxplot(x=raw_notes['pitch'])

plt.subplot(1, 3, 2)
sns.boxplot(x=raw_notes['step'])

plt.subplot(1, 3, 3)
sns.boxplot(x=raw_notes['duration'])

#Scatter Plot Matrix
sns.pairplot(raw_notes[['pitch', 'step', 'duration']])

#Time-Series Plot
plt.figure(figsize=[15, 5])
plt.plot(raw_notes['start'], raw_notes['pitch'], label='Pitch Over Time')
plt.xlabel('Time')
plt.ylabel('Pitch')
plt.title('Pitch vs Time')
plt.show()

#Heatmap of Correlations
corr = raw_notes[['pitch', 'step', 'duration']].corr()
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")

# Violin Plots
plt.figure(figsize=[15, 3])
sns.violinplot(x="pitch", data=raw_notes)
sns.violinplot(x="step", data=raw_notes)
sns.violinplot(x="duration", data=raw_notes)

#Cumulative Distribution Function (CDF) Plot
plt.figure(figsize=[15, 5])
sns.ecdfplot(raw_notes['pitch'], label="Pitch")
sns.ecdfplot(raw_notes['step'], label="Step")
sns.ecdfplot(raw_notes['duration'], label="Duration")
plt.legend()
plt.title('Cumulative Distribution Function')
plt.show()

#Bar Plot of Frequency of Notes
plt.figure(figsize=[15, 5])
sns.barplot(x=raw_notes['pitch'].value_counts().index, y=raw_notes['pitch'].value_counts().values)
plt.title('Frequency of Each Pitch')
plt.xlabel('Pitch')
plt.ylabel('Frequency')
plt.show()

# Transition Matrix of Notes
transition_matrix = np.zeros((max(raw_notes['pitch']) + 1, max(raw_notes['pitch']) + 1))

for i in range(len(raw_notes) - 1):
    current_note = raw_notes['pitch'].iloc[i]
    next_note = raw_notes['pitch'].iloc[i + 1]
    transition_matrix[current_note, next_note] += 1

sns.heatmap(transition_matrix, annot=True, cmap="Blues")
plt.title('Transition Matrix of Notes')
plt.show()

def notes_to_midi(
  notes: pd.DataFrame,
  out_file: str,
  instrument_name: str,
  velocity: int = 100,  # note loudness
) -> pretty_midi.PrettyMIDI:

  pm = pretty_midi.PrettyMIDI()
  instrument = pretty_midi.Instrument(
      program=pretty_midi.instrument_name_to_program(
          instrument_name))

  prev_start = 0
  for i, note in notes.iterrows():
    start = float(prev_start + note['step'])
    end = float(start + note['duration'])
    note = pretty_midi.Note(
        velocity=velocity,
        pitch=int(note['pitch']),
        start=start,
        end=end,
    )
    instrument.notes.append(note)
    prev_start = start

  pm.instruments.append(instrument)
  pm.write(out_file)
  return pm

example_file = 'example.midi'
example_pm = notes_to_midi(
    raw_notes, out_file=example_file, instrument_name=instrument_name)

display_audio(example_pm)

"""## Creating the training dataset

Creating the training dataset by extracting notes from the MIDI files.
"""

num_files = 5
all_notes = []
for f in filenames[:num_files]:
  notes = midi_to_notes(f)
  all_notes.append(notes)

all_notes = pd.concat(all_notes)

n_notes = len(all_notes)
print('Number of notes parsed:', n_notes)

key_order = ['pitch', 'step', 'duration']
train_notes = np.stack([all_notes[key] for key in key_order], axis=1)

print(train_notes)

notes_ds = tf.data.Dataset.from_tensor_slices(train_notes)
notes_ds.element_spec

#does not include max_Velocity
def create_sequences(
    dataset: tf.data.Dataset,
    seq_length: int,
    vocab_size = 128,
) -> tf.data.Dataset:
  """Returns TF Dataset of sequence and label examples."""
  seq_length = seq_length+1

  # Take 1 extra for the labels
  windows = dataset.window(seq_length, shift=1, stride=1,
                              drop_remainder=True)

  # `flat_map` flattens the" dataset of datasets" into a dataset of tensors
  flatten = lambda x: x.batch(seq_length, drop_remainder=True)
  sequences = windows.flat_map(flatten)

  # Normalize note pitch
  def scale_pitch(x):
    x = x/[vocab_size,1.0,1.0]
    return x

  # Split the labels
  def split_labels(sequences):
    inputs = sequences[:-1]
    labels_dense = sequences[-1]
    labels = {key:labels_dense[i] for i,key in enumerate(key_order)}

    return scale_pitch(inputs), labels

  return sequences.map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)

def create_sequences(
    dataset: tf.data.Dataset,
    seq_length: int,
    vocab_size=128,
) -> tf.data.Dataset:
    """Returns TF Dataset of sequence and label examples."""
    seq_length = seq_length + 1

    # Take 1 extra for the labels
    # Create overlapping windows of length seq_length
    windows = dataset.window(seq_length, shift=1, stride=1, drop_remainder=True)

    # `flat_map` flattens the "dataset of datasets" into a dataset of tensors
    flatten = lambda x: x.batch(seq_length, drop_remainder=True)
    sequences = windows.flat_map(flatten)

    # Normalize note pitch and velocity
    def scale_features(x):
        x = x / [vocab_size, 1.0, 1.0]
        return x

    # Split the labels
    def split_labels(sequences):
        inputs = sequences[:-1]
        labels_dense = sequences[-1]
        labels = {key: labels_dense[i] for i, key in enumerate(key_order)}

        return scale_features(inputs), labels

    return sequences.map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)

seq_length = 25
vocab_size = 128
seq_ds = create_sequences(notes_ds, seq_length, vocab_size)
seq_ds.element_spec

"""The shape of the dataset is ```(100,1)```, meaning that the model will take 100 notes as input, and learn to predict the following note as output."""

for seq, target in seq_ds.take(1):
  print('sequence shape:', seq.shape)
  print('sequence elements (first 10):', seq[0: 10])
  print()
  print('target:', target)

"""Batch the examples, and configure the dataset for performance."""

batch_size = 64
buffer_size = n_notes - seq_length  # the number of items in the dataset
train_ds = (seq_ds
            .shuffle(buffer_size)
            .batch(batch_size, drop_remainder=True)
            .cache()
            .prefetch(tf.data.experimental.AUTOTUNE))

train_ds.element_spec

"""## Create and train the model

The model will have three outputs, one for each note variable. For `step` and `duration`, you will use a custom loss function based on mean squared error that encourages the model to output non-negative values.
"""

def mse_with_positive_pressure(y_true: tf.Tensor, y_pred: tf.Tensor):
  mse = (y_true - y_pred) ** 2
  positive_pressure = 10 * tf.maximum(-y_pred, 0.0)
  return tf.reduce_mean(mse + positive_pressure)

input_shape = (seq_length, 3)
learning_rate = 0.005

inputs = tf.keras.Input(input_shape)
x = tf.keras.layers.LSTM(128)(inputs)

outputs = {
  'pitch': tf.keras.layers.Dense(128, name='pitch')(x),
  'step': tf.keras.layers.Dense(1, name='step')(x),
  'duration': tf.keras.layers.Dense(1, name='duration')(x),
}

model = tf.keras.Model(inputs, outputs)

loss = {
      'pitch': tf.keras.losses.SparseCategoricalCrossentropy(
          from_logits=True),
      'step': mse_with_positive_pressure,
      'duration': mse_with_positive_pressure,
}

optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

model.compile(loss=loss, optimizer=optimizer)

model.summary()

"""Testing the `model.evaluate` function, you can see that the `pitch` loss is significantly greater than the `step` and `duration` losses.
Note that `loss` is the total loss computed by summing all the other losses and is currently dominated by the `pitch` loss.
"""

losses = model.evaluate(train_ds, return_dict=True)
losses

"""One way balance this is to use the `loss_weights` argument to compile:"""

model.compile(
    loss=loss,
    loss_weights={
        'pitch': 0.05,
        'step': 1.0,
        'duration':1.0,
    },
    optimizer=optimizer,
)

"""The `loss` then becomes the weighted sum of the individual losses."""

model.evaluate(train_ds, return_dict=True)

"""Train the model."""

callbacks = [
    tf.keras.callbacks.ModelCheckpoint(
        # Update the filepath to end with '.weights.h5'
        filepath='./training_checkpoints/ckpt_{epoch}.weights.h5',
        save_weights_only=True),
    tf.keras.callbacks.EarlyStopping(
        monitor='loss',
        patience=5,
        verbose=1,
        restore_best_weights=True),
]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #add seed
# epochs = 50
# 
# history = model.fit(
#     train_ds,
#     epochs=epochs,
#     callbacks=callbacks,
# )

plt.plot(history.epoch, history.history['loss'], label='total loss')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# epochs = 80
# 
# history = model.fit(
#     train_ds,
#     epochs=epochs,
#     callbacks=callbacks,
# )

plt.plot(history.epoch, history.history['loss'], label='total loss')
plt.show()

# Plot the total loss during training
plt.figure(figsize=(10, 6))
plt.plot(history.epoch, history.history['loss'], label='Total Loss', color='b', marker='o')

# Adding titles and labels
plt.title('Learning Curve - Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')

# Displaying the grid for easier interpretation
plt.grid(True)

# Adding legend
plt.legend()

# Show the plot
plt.show()

def mse_with_positive_pressure(y_true: tf.Tensor, y_pred: tf.Tensor):
    mse = (y_true - y_pred) ** 2
    positive_pressure = 10 * tf.maximum(-y_pred, 0.0)
    return tf.reduce_mean(mse + positive_pressure)

# Define model architecture
input_shape = (seq_length, 3)  # Adjust according to your data
learning_rate = 0.005

inputs = tf.keras.Input(input_shape)
x = tf.keras.layers.LSTM(128)(inputs)

outputs = {
    'pitch': tf.keras.layers.Dense(128, name='pitch')(x),
    'step': tf.keras.layers.Dense(1, name='step')(x),
    'duration': tf.keras.layers.Dense(1, name='duration')(x),
}

model = tf.keras.Model(inputs, outputs)

# Define losses
loss = {
    'pitch': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    'step': mse_with_positive_pressure,
    'duration': mse_with_positive_pressure,
}

# Define optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

model.compile(loss=loss, optimizer=optimizer)

# Show model summary
model.summary()

# Define model evaluation
losses = model.evaluate(train_ds, return_dict=True)
print(losses)

# Recompile the model with weighted losses
model.compile(
    loss=loss,
    loss_weights={
        'pitch': 0.05,
        'step': 1.0,
        'duration': 1.0,
    },
    optimizer=optimizer,
)

# Callbacks
callbacks = [
    tf.keras.callbacks.ModelCheckpoint(
        filepath='./training_checkpoints/ckpt_{epoch}.weights.h5',
        save_weights_only=True),
    tf.keras.callbacks.EarlyStopping(
        monitor='loss',
        patience=5,
        verbose=1,
        restore_best_weights=True),
]

# Training the model
epochs = 50
history = model.fit(
    train_ds,
    epochs=epochs,
    callbacks=callbacks,
)

# Plotting loss and accuracy graphs
plt.figure(figsize=(10, 6))

# Plot the total loss during training
plt.subplot(2, 1, 1)  # First subplot (Loss)
plt.plot(history.epoch, history.history['loss'], label='Total Loss', color='b', marker='o')
plt.title('Learning Curve - Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()

# Plot the accuracy for the 'pitch' output
# Note: Accuracy metric for classification (pitch) is calculated during training
plt.subplot(2, 1, 2)  # Second subplot (Accuracy)
if 'pitch_accuracy' in history.history:
    plt.plot(history.epoch, history.history['pitch_accuracy'], label='Pitch Accuracy', color='g', marker='x')
    plt.title('Accuracy Curve - Pitch vs Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.grid(True)
    plt.legend()
else:
    print("Accuracy metric for 'pitch' not available in history.")

# Show the plot
plt.tight_layout()
plt.show()

"""## Generate notes"""

def predict_next_note(
    notes: np.ndarray,
    model: tf.keras.Model,
    temperature: float = 1.0) -> tuple[int, float, float]:
  """Generates a note as a tuple of (pitch, step, duration), using a trained sequence model."""

  assert temperature > 0

  # Add batch dimension
  inputs = tf.expand_dims(notes, 0)

  predictions = model.predict(inputs)
  pitch_logits = predictions['pitch']
  step = predictions['step']
  duration = predictions['duration']

  pitch_logits /= temperature
  pitch = tf.random.categorical(pitch_logits, num_samples=1)
  pitch = tf.squeeze(pitch, axis=-1)
  duration = tf.squeeze(duration, axis=-1)
  step = tf.squeeze(step, axis=-1)

  # `step` and `duration` values should be non-negative
  step = tf.maximum(0, step)
  duration = tf.maximum(0, duration)

  return int(pitch), float(step), float(duration)

"""Now generate some notes. You can play around with temperature and the starting sequence in `next_notes` and see what happens."""

temperature = 2.0
num_predictions = 120

sample_notes = np.stack([raw_notes[key] for key in key_order], axis=1)

# The initial sequence of notes; pitch is normalized similar to training
# sequences
input_notes = (
    sample_notes[:seq_length] / np.array([vocab_size, 1, 1]))

generated_notes = []
prev_start = 0
for _ in range(num_predictions):
  pitch, step, duration = predict_next_note(input_notes, model, temperature)
  start = prev_start + step
  end = start + duration
  input_note = (pitch, step, duration)
  generated_notes.append((*input_note, start, end))
  input_notes = np.delete(input_notes, 0, axis=0)
  input_notes = np.append(input_notes, np.expand_dims(input_note, 0), axis=0)
  prev_start = start

generated_notes = pd.DataFrame(
    generated_notes, columns=(*key_order, 'start', 'end'))

generated_notes.head(10)

out_file = 'output.mid'
out_pm = notes_to_midi(
    generated_notes, out_file=out_file, instrument_name=instrument_name)
display_audio(out_pm)

"""We can also download the audio file by adding the two lines below:

```
from google.colab import files
files.download(out_file)
```

Visualize the generated notes.
"""

plot_piano_roll(generated_notes)

"""Check the distributions of `pitch`, `step` and `duration`."""

plot_distributions(generated_notes)

"""##F1 Score Calculation"""

from sklearn.metrics import f1_score
import numpy as np

def discretize_pitch(pitch, bins=12):
    """ Discretize pitch into bins (e.g., 12 for chromatic scale). """
    return int(pitch // (128 / bins))  # Assuming pitch is between 0 and 127 for MIDI

def discretize_step(step, threshold=0.1):
    """ Discretize step values (time between notes) to a binary classification. """
    return 1 if step > threshold else 0  # 1 for step > threshold, else 0

def discretize_duration(duration, threshold=0.1):
    """ Discretize duration values to a binary classification. """
    return 1 if duration > threshold else 0  # 1 for duration > threshold, else 0

# Example data: true values and generated values
true_pitches = [86, 87, 64, 85, 67]
generated_pitches = [86, 88, 85, 85, 85]

true_steps = [0.000000, 0.041667, 0.048177, 0.865434, 0.113281]
generated_steps = [0.051559, 0.769668, 0.917342, 0.944122, 0.149867]

true_durations = [0.194010, 0.000000, 0.000000, 0.00000, 0.122396]
generated_durations = [0.09445, 0.00000, 0.00000, 0.00000, 0.00000]

# Discretize the pitches, steps, and durations into bins or binary values
true_pitches_discretized = [discretize_pitch(p) for p in true_pitches]
generated_pitches_discretized = [discretize_pitch(p) for p in generated_pitches]

true_steps_discretized = [discretize_step(s) for s in true_steps]
generated_steps_discretized = [discretize_step(s) for s in generated_steps]

true_durations_discretized = [discretize_duration(d) for d in true_durations]
generated_durations_discretized = [discretize_duration(d) for d in generated_durations]

# Calculate F1 scores for pitch, step, and duration classification
f1_pitch = f1_score(true_pitches_discretized, generated_pitches_discretized, average='weighted')
f1_step = f1_score(true_steps_discretized, generated_steps_discretized, average='weighted')
f1_duration = f1_score(true_durations_discretized, generated_durations_discretized, average='weighted')

# Print the F1 scores
print(f"F1 Score for pitch: {f1_pitch}")
print(f"F1 Score for step: {f1_step}")
print(f"F1 Score for duration: {f1_duration}")

"""##Confusion Matrix"""

import pandas as pd
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Example: True and predicted data for pitch, step, and duration (continuous values)
true_data = {
    'pitch': [86, 87, 64, 85, 67],
    'step': [0.000000, 0.041667, 0.048177, 0.865434, 0.113281],
    'duration': [0.194010, 0.000000, 0.000000, 0.00000, 0.122396]
}

generated_data = {
    'pitch': [86, 88, 85, 85, 85],
    'step': [0.051559, 0.769668, 0.917342, 0.944122, 0.149867],
    'duration': [0.09445, 0.00000, 0.00000, 0.00000, 0.00000]
}

# Function to discretize continuous pitch values (adjusted)
def discretize_pitch(pitch, min_pitch=64, max_pitch=88, bins=6):
    bin_width = (max_pitch - min_pitch) / bins
    return int((pitch - min_pitch) // bin_width)  # Create dynamic bins based on min and max pitch

# Function to discretize step size (binary classification)
def discretize_step(step, threshold=0.1):
    return 1 if step > threshold else 0  # Binary classification for step size

# Function to discretize duration (binary classification)
def discretize_duration(duration, threshold=0.1):
    return 1 if duration > threshold else 0  # Binary classification for duration

# Discretize the data for pitch, step, and duration
true_pitches = [discretize_pitch(p) for p in true_data['pitch']]
generated_pitches = [discretize_pitch(p) for p in generated_data['pitch']]

true_steps = [discretize_step(s) for s in true_data['step']]
generated_steps = [discretize_step(s) for s in generated_data['step']]

true_durations = [discretize_duration(d) for d in true_data['duration']]
generated_durations = [discretize_duration(d) for d in generated_data['duration']]

# Compute confusion matrix for pitch, step, and duration
cm_pitch = confusion_matrix(true_pitches, generated_pitches)
cm_step = confusion_matrix(true_steps, generated_steps)
cm_duration = confusion_matrix(true_durations, generated_durations)

# Plot the confusion matrix for pitch
plt.figure(figsize=(6, 5))
sns.heatmap(cm_pitch, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.title('Pitch Confusion Matrix')
plt.show()

# Plot the confusion matrix for step
plt.figure(figsize=(6, 5))
sns.heatmap(cm_step, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.title('Step Confusion Matrix')
plt.show()

# Plot the confusion matrix for duration
plt.figure(figsize=(6, 5))
sns.heatmap(cm_duration, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.title('Duration Confusion Matrix')
plt.show()

correlation_matrix = true_df.corrwith(generated_df)
print(correlation_matrix)

"""###How to Implement Autoencoders in Python and Keras

"""

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose

# Autoencoder class definition
class Autoencoder:
    def __init__(self, input_shape, conv_filters, conv_kernels, conv_strides, latent_space_dim):
        self.input_shape = input_shape
        self.conv_filters = conv_filters
        self.conv_kernels = conv_kernels
        self.conv_strides = conv_strides
        self.latent_space_dim = latent_space_dim
        self.model = None
        self.encoder = None
        self.decoder = None
        self._build()

    def _build(self):
        # Encoder
        encoder_input = Input(shape=self.input_shape, name="encoder_input")
        x = encoder_input
        for filters, kernel, stride in zip(self.conv_filters, self.conv_kernels, self.conv_strides):
            x = Conv2D(filters=filters, kernel_size=kernel, strides=stride, padding="same", activation="relu")(x)
        shape_before_flattening = x.shape[1:]
        x = Flatten()(x)
        latent_space = Dense(self.latent_space_dim, name="latent_space")(x)
        self.encoder = Model(encoder_input, latent_space, name="encoder")

        # Decoder
        decoder_input = Input(shape=(self.latent_space_dim,), name="decoder_input")
        x = Dense(np.prod(shape_before_flattening))(decoder_input)
        x = Reshape(shape_before_flattening)(x)
        for filters, kernel, stride in zip(reversed(self.conv_filters), reversed(self.conv_kernels), reversed(self.conv_strides)):
            x = Conv2DTranspose(filters=filters, kernel_size=kernel, strides=stride, padding="same", activation="relu")(x)
        decoder_output = Conv2DTranspose(filters=1, kernel_size=3, activation="sigmoid", padding="same", name="decoder_output")(x)
        self.decoder = Model(decoder_input, decoder_output, name="decoder")

        # Autoencoder
        autoencoder_input = encoder_input
        autoencoder_output = self.decoder(self.encoder(autoencoder_input))
        self.model = Model(autoencoder_input, autoencoder_output, name="autoencoder")

    def summary(self):
        self.encoder.summary()
        self.decoder.summary()
        self.model.summary()

    def compile(self, learning_rate):
        self.model.compile(optimizer=Adam(learning_rate=learning_rate), loss="mse")

    def train(self, x_train, batch_size, epochs):
        history = self.model.fit(
            x_train, x_train,
            batch_size=batch_size,
            epochs=epochs,
            shuffle=True
        )
        return history

# Constants
LEARNING_RATE = 0.0005
BATCH_SIZE = 32
EPOCHS = 20

# Function to load and preprocess MIDI dataset
def load_midi_dataset(filenames):
    """Loads and preprocesses the MIDI dataset."""
    print('Number of files:', len(filenames))
    dataset = []

    for file in filenames:
        # Replace this placeholder with actual MIDI-to-matrix conversion logic
        midi_data = np.random.rand(128, 128)  # Example placeholder for MIDI processing
        dataset.append(midi_data)

    dataset = np.array(dataset)
    dataset = dataset.astype("float32") / np.max(dataset)  # Normalize
    dataset = np.expand_dims(dataset, axis=-1)  # Add channel dimension

    return dataset

# Function to train the autoencoder
def train_autoencoder_model(x_train, learning_rate, batch_size, epochs):
    """Initializes, compiles, and trains the Autoencoder."""
    autoencoder = Autoencoder(
        input_shape=(128, 128, 1),  # Update based on your MIDI encoding shape
        conv_filters=[32, 64, 64, 64],
        conv_kernels=[3, 3, 3, 3],
        conv_strides=[1, 2, 2, 1],
        latent_space_dim=16  # Adjust as needed
    )
    autoencoder.summary()
    autoencoder.compile(learning_rate)
    history = autoencoder.train(x_train, batch_size, epochs)
    return autoencoder, history

# Function to plot training loss graph
def plot_training_loss(history):
    """Plots the training loss graph."""
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.title('Training Loss Over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid()
    plt.show()

# List of filenames (provide this as input to the script)
# Example: filenames = ["file1.mid", "file2.mid", ...]
filenames = ["example1.mid", "example2.mid", "example3.mid"]

# Load and preprocess the dataset
x_train = load_midi_dataset(filenames)

# Train the autoencoder
autoencoder, history = train_autoencoder_model(x_train, LEARNING_RATE, BATCH_SIZE, EPOCHS)

# Plot training loss
plot_training_loss(history)

def predict_next_note(
    notes: np.ndarray,
    model: tf.keras.Model,
    temperature: float = 1.0) -> tuple[int, float, float]:
  """Generates a note as a tuple of (pitch, step, duration), using a trained sequence model."""

  assert temperature > 0

  # Add batch dimension
  inputs = tf.expand_dims(notes, 0)

  predictions = model.predict(inputs)
  pitch_logits = predictions['pitch']
  step = predictions['step']
  duration = predictions['duration']

  pitch_logits /= temperature
  pitch = tf.random.categorical(pitch_logits, num_samples=1)
  pitch = tf.squeeze(pitch, axis=-1)
  duration = tf.squeeze(duration, axis=-1)
  step = tf.squeeze(step, axis=-1)

  # `step` and `duration` values should be non-negative
  step = tf.maximum(0, step)
  duration = tf.maximum(0, duration)

  return int(pitch), float(step), float(duration)

temperature = 2.0
num_predictions = 120

sample_notes = np.stack([raw_notes[key] for key in key_order], axis=1)

# The initial sequence of notes; pitch is normalized similar to training
# sequences
input_notes = (
    sample_notes[:seq_length] / np.array([vocab_size, 1, 1]))

generated_notes = []
prev_start = 0
for _ in range(num_predictions):
  pitch, step, duration = predict_next_note(input_notes, model, temperature)
  start = prev_start + step
  end = start + duration
  input_note = (pitch, step, duration)
  generated_notes.append((*input_note, start, end))
  input_notes = np.delete(input_notes, 0, axis=0)
  input_notes = np.append(input_notes, np.expand_dims(input_note, 0), axis=0)
  prev_start = start

generated_notes = pd.DataFrame(
    generated_notes, columns=(*key_order, 'start', 'end'))

generated_notes.head(10)

out_file = 'output.mid'
out_pm = notes_to_midi(
    generated_notes, out_file=out_file, instrument_name=instrument_name)
display_audio(out_pm)

plot_piano_roll(generated_notes)

import numpy as np
from sklearn.metrics import f1_score, mean_squared_error
from scipy.stats import entropy

# Example: Assume raw_notes and generated_notes are DataFrames with pitch, step, and duration columns
real_pitch = raw_notes['pitch'].values
gen_pitch = generated_notes['pitch'].values

real_step = raw_notes['step'].values
gen_step = generated_notes['step'].values

real_duration = raw_notes['duration'].values
gen_duration = generated_notes['duration'].values

# Assuming real_notes and generated_notes are binary matrices for note presence
min_len = min(len(real_pitch), len(gen_pitch))  # Find the minimum length

real_notes = np.zeros((128, min_len))
generated_notes = np.zeros((128, min_len))

for i, pitch in enumerate(real_pitch[:min_len]):  # Iterate up to min_len
    real_notes[pitch, i] = 1

for i, pitch in enumerate(gen_pitch[:min_len]):  # Iterate up to min_len
    generated_notes[pitch, i] = 1

# Flatten matrices for comparison
real_flat = real_notes.flatten()
gen_flat = generated_notes.flatten()

# Calculate F1 Score
f1 = f1_score(real_flat, gen_flat, average='weighted')
print(f"F1 Score: {f1}")

# MSE for pitch, step, and duration
min_len = min(len(real_pitch), len(gen_pitch)) # Get minimum length

# Slice arrays to ensure they have the same length
mse_pitch = mean_squared_error(real_pitch[:min_len], gen_pitch[:min_len])
mse_step = mean_squared_error(real_step[:min_len], gen_step[:min_len])
mse_duration = mean_squared_error(real_duration[:min_len], gen_duration[:min_len])

print(f"MSE (Pitch): {mse_pitch}")
print(f"MSE (Step): {mse_step}")
print(f"MSE (Duration): {mse_duration}")

# Normalize distributions
real_pitch_dist = np.bincount(real_pitch, minlength=128) / len(real_pitch)
gen_pitch_dist = np.bincount(gen_pitch, minlength=128) / len(gen_pitch)

real_step_dist = np.histogram(real_step, bins=20, density=True)[0]
gen_step_dist = np.histogram(gen_step, bins=20, density=True)[0]

real_duration_dist = np.histogram(real_duration, bins=20, density=True)[0]
gen_duration_dist = np.histogram(gen_duration, bins=20, density=True)[0]

epsilon = 1e-10
kl_pitch = entropy(real_pitch_dist + epsilon, gen_pitch_dist + epsilon)
kl_step = entropy(real_step_dist + epsilon, gen_step_dist + epsilon)
kl_duration = entropy(real_duration_dist + epsilon, gen_duration_dist + epsilon)

print(f"KL Divergence (Pitch): {kl_pitch}")
print(f"KL Divergence (Step): {kl_step}")
print(f"KL Divergence (Duration): {kl_duration}")

# Normalize distributions
real_pitch_dist = np.bincount(real_pitch, minlength=128)
gen_pitch_dist = np.bincount(gen_pitch, minlength=128)

# Ensure distributions sum to 1
real_pitch_dist = real_pitch_dist / np.sum(real_pitch_dist)
gen_pitch_dist = gen_pitch_dist / np.sum(gen_pitch_dist)

# Truncate sequences to the same length
min_len = min(len(real_pitch), len(gen_pitch))
real_pitch = real_pitch[:min_len]
gen_pitch = gen_pitch[:min_len]

# Plot normalized distributions
plt.figure(figsize=(10, 6))
plt.bar(range(128), real_pitch_dist, alpha=0.5, label="Real")
plt.bar(range(128), gen_pitch_dist, alpha=0.5, label="Generated")
plt.title("Pitch Distribution")
plt.xlabel("Pitch (MIDI values)")
plt.ylabel("Probability")
plt.legend()
plt.show()

# Print sum validation
print(f"Sum of Real Pitch Distribution: {np.sum(real_pitch_dist):.4f}")
print(f"Sum of Generated Pitch Distribution: {np.sum(gen_pitch_dist):.4f}")

#Cosine Similarity for Distribution Comparison
from sklearn.metrics.pairwise import cosine_similarity
pitch_similarity = cosine_similarity(real_pitch_dist.reshape(1, -1), gen_pitch_dist.reshape(1, -1))
print(f"Cosine Similarity (Pitch): {pitch_similarity[0][0]}")

#Statistical Testing
from scipy.stats import ks_2samp
ks_stat, p_value_pitch = ks_2samp(real_pitch, gen_pitch)
print(f"KS Statistic (Pitch): {ks_stat}, P-value: {p_value_pitch}")

"""###Autoencoding with LSTM"""

import tensorflow as tf
import numpy as np

# Custom loss function
def mse_with_positive_pressure(y_true: tf.Tensor, y_pred: tf.Tensor):
    mse = (y_true - y_pred) ** 2
    positive_pressure = 10 * tf.maximum(-y_pred, 0.0)
    return tf.reduce_mean(mse + positive_pressure)

# Define the input shape and learning rate
seq_length = 128  # Example sequence length, adjust as needed
input_shape = (seq_length, 3)  # Adjust dimensions based on your input data
learning_rate = 0.005

# Define the encoder-decoder architecture
inputs = tf.keras.Input(shape=input_shape)

# Encoder (LSTM-based)
x = tf.keras.layers.LSTM(128)(inputs)

# Decoder (Output layers for pitch, step, and duration)
outputs = {
    'pitch': tf.keras.layers.Dense(128, name='pitch')(x),
    'step': tf.keras.layers.Dense(1, name='step')(x),
    'duration': tf.keras.layers.Dense(1, name='duration')(x),
}

# Create the model
model = tf.keras.Model(inputs, outputs)

# Define loss functions for each output
loss = {
    'pitch': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    'step': mse_with_positive_pressure,
    'duration': mse_with_positive_pressure,
}

# Define the optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

# Compile the model
model.compile(loss=loss, optimizer=optimizer)

# Show the model summary
model.summary()

# Define the dataset (replace with actual training data)
# train_ds should be a tf.data.Dataset object containing the training data
# This is just a placeholder
# train_ds = ... (load your dataset here)

# Evaluate the model on the training dataset
losses = model.evaluate(train_ds, return_dict=True)
print("Initial evaluation losses:", losses)

# Re-compile the model with custom loss weights
model.compile(
    loss=loss,
    loss_weights={
        'pitch': 0.05,
        'step': 1.0,
        'duration': 1.0,
    },
    optimizer=optimizer,
)

# Evaluate again with weighted losses
losses = model.evaluate(train_ds, return_dict=True)
print("Losses after applying loss weights:", losses)

# Define callbacks
callbacks = [
    tf.keras.callbacks.ModelCheckpoint(
        filepath='./training_checkpoints/ckpt_{epoch}.weights.h5',  # Save model weights
        save_weights_only=True
    ),
    tf.keras.callbacks.EarlyStopping(
        monitor='loss',  # Monitor the loss for early stopping
        patience=5,
        verbose=1,
        restore_best_weights=True
    ),
]

# Train the model
epochs = 50
history = model.fit(
    train_ds,
    epochs=epochs,
    callbacks=callbacks
)

