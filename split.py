# -*- coding: utf-8 -*-
"""SLP_split.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-kfUwztw8F9cfU3XO0kZI5N2XXwmOxpT

#Music Generation

**Dataset** - Piano MIDI files - [MAESTRO dataset](https://magenta.tensorflow.org/datasets/maestro)

###Setup
"""

!pip install pyfluidsynth

!sudo apt install -y fluidsynth

!pip install --upgrade pyfluidsynth

!pip install pretty_midi

import collections
import datetime
import fluidsynth
import glob
import numpy as np
import pathlib
import pandas as pd
import pretty_midi
import seaborn as sns
import tensorflow as tf
from IPython import display
from matplotlib import pyplot as plt
from typing import Optional

import librosa
import librosa.display
import IPython.display as ipd
import glob
import os

seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)

# Sampling rate for audio playback
_SAMPLING_RATE = 16000

"""###Download the Maestro dataset"""

data_dir = pathlib.Path('data/maestro-v2.0.0')
if not data_dir.exists():
  tf.keras.utils.get_file(
      'maestro-v2.0.0-midi.zip',
      origin='https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip',
      extract=True,
      cache_dir='.', cache_subdir='data',
  )

data_dir = pathlib.Path('data/maestro-v2.0.0') # Re-initialize data_dir as a pathlib.Path object
filenames = glob.glob(str(data_dir / '**/*.mid*')) # This line should now work correctly
print('Number of files:', len(filenames))

metadata_csv_path = data_dir / 'maestro-v2.0.0.csv'
metadata_json_path = data_dir / 'maestro-v2.0.0.json'

print(metadata_csv_path.exists(), metadata_json_path.exists())

train_count = 960
val_count = 128
test_count = 128

# Shuffle filenames randomly
np.random.shuffle(filenames)

# Split the filenames into train, validation, and test based on the counts
train_files = filenames[:train_count]
val_files = filenames[train_count:train_count + val_count]
test_files = filenames[train_count + val_count:train_count + val_count + test_count]

# Print the number of files in each split to ensure correctness
print(f"Train files: {len(train_files)}")
print(f"Validation files: {len(val_files)}")
print(f"Test files: {len(test_files)}")

"""###Process a MIDI file"""

sample_file = filenames[1]
print(sample_file)

pm = pretty_midi.PrettyMIDI(sample_file)

#Note Statistics
for i, instrument in enumerate(pm.instruments):
    note_pitches = [note.pitch for note in instrument.notes]
    note_durations = [note.end - note.start for note in instrument.notes]
    print(f"\nInstrument {i+1}:")
    print("  Pitch Range:", min(note_pitches), "-", max(note_pitches))
    print("  Average Note Duration:", sum(note_durations) / len(note_durations) if note_durations else 0)

# Tempo Analysis
tempos, times = pm.get_tempo_changes()

# Check if the tempo data is valid
if len(tempos) == 1 and tempos[0] == 0:
    print("Invalid tempo detected, using estimated tempo.")
    avg_tempo = pm.estimate_tempo()  # Use the estimated tempo
else:
    avg_tempo = sum(tempos) / len(tempos) if tempos else pm.estimate_tempo()  # Fallback to estimate if empty

print(f"Average Tempo: {avg_tempo:.2f} BPM")
print("Tempo Changes (tempos):", tempos)
print("Tempo Change Times (times):", times)

# Time Signature and Key Signature Changes
print("Time Signature Changes:", pm.time_signature_changes)
print("Key Signature Changes:", pm.key_signature_changes)

# Instrument Analysis
print("Instruments:", pm.instruments)  # Lists all instruments in the MIDI file
for i, instrument in enumerate(pm.instruments):
    print(f"\nInstrument {i+1}:")
    print("  Name:", instrument.name)
    print("  Program:", instrument.program)
    print("  Is Drum:", instrument.is_drum)
    print("  Number of Notes:", len(instrument.notes))
    print("  Number of Pitch Bends:", len(instrument.pitch_bends))
    print("  Number of Control Changes:", len(instrument.control_changes))

# General Instrument Summary
print('Number of instruments:', len(pm.instruments))
if pm.instruments:
    instrument = pm.instruments[0]
    instrument_name = pretty_midi.program_to_instrument_name(instrument.program)
    print('First Instrument Name:', instrument_name)

#Tempo Analysis
tempos, times = pm.get_tempo_changes()
avg_tempo = sum(tempos) / len(tempos) if tempos else 0
print(f"Average Tempo: {avg_tempo} BPM")

print("Tempo Changes (tempos):", tempos)
print("Tempo Change Times (times):", times)

print("Default Tempo (pretty_midi default):", pm.estimate_tempo())

def display_audio(pm: pretty_midi.PrettyMIDI, seconds=30):
  waveform = pm.fluidsynth(fs=_SAMPLING_RATE)
  # Take a sample of the generated waveform to mitigate kernel resets
  waveform_short = waveform[:seconds*_SAMPLING_RATE]
  return display.Audio(waveform_short, rate=_SAMPLING_RATE)

display_audio(pm)

# Visualize Note Pitch Distribution
note_pitches = [note.pitch for instrument in pm.instruments for note in instrument.notes]
import matplotlib.pyplot as plt

plt.hist(note_pitches, bins=range(min(note_pitches), max(note_pitches) + 1), color='blue', edgecolor='black')
plt.title("Note Pitch Distribution")
plt.xlabel("Pitch")
plt.ylabel("Frequency")
plt.show()

# Visualize Note Density Over Time
note_start_times = [note.start for instrument in pm.instruments for note in instrument.notes]
plt.hist(note_start_times, bins=50, color='green', edgecolor='black')
plt.title("Note Density Over Time")
plt.xlabel("Time (seconds)")
plt.ylabel("Number of Notes")
plt.show()

# Using music21 to analyze the key
from music21 import converter

midi_data = converter.parse(sample_file)
key = midi_data.analyze('key')
print("Detected Key:", key)

midi_files = glob.glob(str(data_dir/'**/*.mid*'))

midi_files = midi_files[:10]

file_data = []

for file_path in midi_files:
    file_name = os.path.basename(file_path)

    pm = pretty_midi.PrettyMIDI(file_path)

    tempo_changes = pm.get_tempo_changes()
    file_info = {
        "File Name": file_name,
        "Tempo Changes (Count)": len(tempo_changes[0]),
        "Time Signature Changes (Count)": len(pm.time_signature_changes),
        "Key Signature Changes (Count)": len(pm.key_signature_changes),
        "Number of Instruments": len(pm.instruments),
    }

    # Aggregate instrument data
    total_notes = sum(len(inst.notes) for inst in pm.instruments)
    total_pitch_bends = sum(len(inst.pitch_bends) for inst in pm.instruments)
    total_control_changes = sum(len(inst.control_changes) for inst in pm.instruments)

    file_info["Total Notes"] = total_notes
    file_info["Total Pitch Bends"] = total_pitch_bends
    file_info["Total Control Changes"] = total_control_changes

    # Append to the list
    file_data.append(file_info)

# Convert to DataFrame
df = pd.DataFrame(file_data)
print(df)

# Save DataFrame to CSV
df.to_csv("midi_file_summary.csv", index=False)

file_data = []

# Loop over each file
for file_path in midi_files:
    file_name = os.path.basename(file_path)

    pm = pretty_midi.PrettyMIDI(file_path)

    # Get tempo changes
    tempo_changes, _ = pm.get_tempo_changes()
    tempo_std = np.std(tempo_changes) if len(tempo_changes) > 1 else 0  # Standard deviation of tempo changes

    # Collect other information
    file_info = {
        "File Name": file_name,
        "Tempo Changes (Count)": len(tempo_changes),
        "Tempo Standard Deviation": tempo_std,
        "Time Signature Changes (Count)": len(pm.time_signature_changes),
        "Key Signature Changes (Count)": len(pm.key_signature_changes),
        "Number of Instruments": len(pm.instruments),
    }

    # Aggregate instrument data
    total_notes = sum(len(inst.notes) for inst in pm.instruments)
    total_pitch_bends = sum(len(inst.pitch_bends) for inst in pm.instruments)
    total_control_changes = sum(len(inst.control_changes) for inst in pm.instruments)

    file_info["Total Notes"] = total_notes
    file_info["Total Pitch Bends"] = total_pitch_bends
    file_info["Total Control Changes"] = total_control_changes

    file_data.append(file_info)

# Convert to DataFrame
df = pd.DataFrame(file_data)

# Calculate the mean and standard deviation for each column
numeric_columns = df.select_dtypes(include=np.number).columns
means = df[numeric_columns].mean()
std_devs = df[numeric_columns].std()

# Applying Interquartile Range (IQR) method to filter outliers
Q1 = df[numeric_columns].quantile(0.25)
Q3 = df[numeric_columns].quantile(0.75)
IQR = Q3 - Q1

# Filter out files that are outliers for each column
filtered_df = df[~((df[numeric_columns] < (Q1 - 1.5 * IQR)) | (df[numeric_columns] > (Q3 + 1.5 * IQR))).any(axis=1)]

# Plotting the range of values for each property
plt.figure(figsize=(12, 6))

# Tempo Changes (filtered)
plt.subplot(1, 3, 1)
plt.bar(filtered_df.index, filtered_df["Tempo Changes (Count)"], color='blue')
plt.title("Tempo Changes")
plt.xticks(rotation=45)
plt.ylabel("Count")

# Total Notes (filtered)
plt.subplot(1, 3, 2)
plt.bar(filtered_df.index, filtered_df["Total Notes"], color='orange')
plt.title("Total Notes")
plt.xticks(rotation=45)
plt.ylabel("Count")

# Number of Instruments (filtered)
plt.subplot(1, 3, 3)
plt.bar(filtered_df.index, filtered_df["Number of Instruments"], color='green')
plt.title("Number of Instruments")
plt.xticks(rotation=45)
plt.ylabel("Count")

plt.tight_layout()
plt.show()

# Show statistics for filtered files
print("Mean Values for Filtered Data:")
print(means)

print("\nStandard Deviations for Filtered Data:")
print(std_devs)

print("\nInterquartile Range (IQR) for Filtered Data:")
print(IQR)

print("\nNotes in first track:")
for note in pm.instruments[0].notes[:5]:
    print(f"  Start: {note.start}, End: {note.end}, Pitch: {note.pitch}")

"""###Visualising audio signal in the time domain"""

# Assuming filenames is a list of MIDI file paths
#filenames = [f"file_path_{i}.midi" for i in range(1, 21)] #This line creates placeholders, not actual files
# Replace with the actual file paths from the Maestro Dataset
filenames = glob.glob(str(data_dir/'**/*.mid*'))
filenames = filenames[:20] #Get the first 20 MIDI files in the dataset

# Function to convert MIDI to audio
def midi_to_audio(midi_file):
    midi_data = pretty_midi.PrettyMIDI(midi_file)
    audio_signal = midi_data.fluidsynth()  # Uses default soundfont to synthesize audio
    return audio_signal

# List and print the first 3 files (for reference)
for i, file in enumerate(filenames[:21]):
    print(f"File {i + 1}: {file}")

# Loop to process the first 20 files
for idx, midi_file in enumerate(filenames[:21]):
    # Convert MIDI to audio
    audio_signal = midi_to_audio(midi_file)

    # Set the sample rate (assuming 22050 Hz)
    sr = 22050
    signal = audio_signal

    # Print information about the audio
    print(f"Audio {idx+1}: {midi_file}")
    print(f"Audio Length: {len(signal)} samples")

    # Duration in seconds of one sample
    sample_duration = 1 / sr
    print(f"One sample lasts for {sample_duration:6f} seconds")

    # Total number of samples in the audio file
    tot_samples = len(signal)
    print(f"Total Samples: {tot_samples}")

    # Duration of the audio in seconds
    duration = 1 / sr * tot_samples
    print(f"Audio Duration: {duration:.2f} seconds")

    # Plotting the waveform
    plt.figure(figsize=(15, 5))
    librosa.display.waveshow(signal, alpha=0.5, sr=sr)
    plt.ylim((-1, 1))
    plt.title(f"Audio {idx+1} - {midi_file}")
    plt.xlabel("Time (seconds)")
    plt.ylabel("Amplitude")
    plt.show()

"""###Calculating and Visualising amplitude envelope"""

# Assuming data_dir is the path to your MIDI files folder
filenames = glob.glob(str(data_dir/'**/*.mid*'))
filenames = filenames[:20]  # Get the first 20 MIDI files in the dataset

# Function to convert MIDI to audio
def midi_to_audio(midi_file):
    midi_data = pretty_midi.PrettyMIDI(midi_file)
    audio_signal = midi_data.fluidsynth()  # Uses default soundfont to synthesize audio
    return audio_signal

# Function to calculate amplitude envelope
FRAME_SIZE = 1024
HOP_LENGTH = 512

def amplitude_envelope(signal, frame_size, hop_length):
    """Calculate the amplitude envelope of a signal with a given frame size and hop length."""
    amplitude_envelope = []

    # Calculate amplitude envelope for each frame
    for i in range(0, len(signal), hop_length):
        amplitude_envelope_current_frame = max(signal[i:i+frame_size])
        amplitude_envelope.append(amplitude_envelope_current_frame)

    return np.array(amplitude_envelope)

# Loop to process the first 20 files
for idx, midi_file in enumerate(filenames[:20]):
    # Convert MIDI to audio
    audio_signal = midi_to_audio(midi_file)

    # Set the sample rate (assuming 22050 Hz)
    sr = 22050
    signal = audio_signal

    # Print information about the audio
    print(f"Audio {idx+1}: {midi_file}")
    print(f"Audio Length: {len(signal)} samples")

    # Duration in seconds of one sample
    sample_duration = 1 / sr
    print(f"One sample lasts for {sample_duration:6f} seconds")

    # Total number of samples in the audio file
    tot_samples = len(signal)
    print(f"Total Samples: {tot_samples}")

    # Duration of the audio in seconds
    duration = 1 / sr * tot_samples
    print(f"Audio Duration: {duration:.2f} seconds")

    # Calculate the amplitude envelope for the signal
    amplitude_env = amplitude_envelope(signal, FRAME_SIZE, HOP_LENGTH)

    # Generate the time vector for the amplitude envelope
    frames = range(len(amplitude_env))
    t = librosa.frames_to_time(frames, hop_length=HOP_LENGTH)

    # Plotting the waveform and amplitude envelope
    plt.figure(figsize=(15, 7))

    # Plot the waveform
    plt.subplot(2, 1, 1)
    librosa.display.waveshow(signal, alpha=0.5, sr=sr)
    plt.ylim((-1, 1))
    plt.title(f"Waveform of Audio {idx+1} - {midi_file}")
    plt.xlabel("Time (seconds)")
    plt.ylabel("Amplitude")

    # Plot the amplitude envelope
    plt.subplot(2, 1, 2)
    plt.plot(t, amplitude_env, color='r', alpha=0.7)
    plt.title(f"Amplitude Envelope of Audio {idx+1} - {midi_file}")
    plt.xlabel("Time (seconds)")
    plt.ylabel("Amplitude")

    plt.tight_layout()
    plt.show()

"""###Extract notes"""

for i, note in enumerate(instrument.notes[:10]):
  note_name = pretty_midi.note_number_to_name(note.pitch)
  duration = note.end - note.start
  print(f'{i}: pitch={note.pitch}, note_name={note_name},'
        f' duration={duration:.4f}')

"""####tempo, key signature, time signature across all midi files"""

def midi_to_notes(midi_file: str) -> pd.DataFrame:
    pm = pretty_midi.PrettyMIDI(midi_file)
    instrument = pm.instruments[0]
    notes = collections.defaultdict(list)

    # Sort the notes by start time
    sorted_notes = sorted(instrument.notes, key=lambda note: note.start)
    prev_start = sorted_notes[0].start

    for note in sorted_notes:
        start = note.start
        end = note.end
        notes['pitch'].append(note.pitch)  # MIDI pitch (e.g., 60 = Middle C)
        notes['start'].append(start)  # Start time of the note
        notes['end'].append(end)  # End time of the note
        notes['step'].append(start - prev_start)  # Time step from the previous note
        notes['duration'].append(end - start)  # Duration of the note

        prev_start = start

    # Add time signature as 4/4
    time_signature = (4, 4)

    # Convert the dictionary to a DataFrame
    notes_df = pd.DataFrame({name: np.array(value) for name, value in notes.items()})

    # Add a column for time signature
    notes_df['time_signature'] = str(time_signature)  # 4/4 as the fixed time signature

    return notes_df

raw_notes = midi_to_notes(sample_file)
raw_notes.head()

def midi_to_notes(midi_file: str) -> pd.DataFrame:
    pm = pretty_midi.PrettyMIDI(midi_file)

    # Extract the first instrument (if there are multiple, you can extend to handle all)
    instrument = pm.instruments[0]

    # Get the instrument name (e.g., "Acoustic Grand Piano")
    instrument_name = pretty_midi.program_to_instrument_name(instrument.program)

    # Initialize a dictionary to store the notes
    notes = collections.defaultdict(list)

    # Sort the notes by start time
    sorted_notes = sorted(instrument.notes, key=lambda note: note.start)
    prev_start = sorted_notes[0].start

    for note in sorted_notes:
        start = note.start
        end = note.end
        notes['pitch'].append(note.pitch)  # MIDI pitch (e.g., 60 = Middle C)
        notes['start'].append(start)  # Start time of the note
        notes['end'].append(end)  # End time of the note
        notes['step'].append(start - prev_start)  # Time step from the previous note
        notes['duration'].append(end - start)  # Duration of the note
        notes['velocity'].append(note.velocity)  # Velocity of the note

        prev_start = start

    # Add time signature as 4/4
    time_signature = (4, 4)

    # Convert the dictionary to a DataFrame
    notes_df = pd.DataFrame({name: np.array(value) for name, value in notes.items()})

    # Add a column for time signature and instrument name
    notes_df['time_signature'] = str(time_signature)  # 4/4 as the fixed time signature
    notes_df['instrument_name'] = instrument_name  # Add instrument name

    return notes_df

raw_notes = midi_to_notes(sample_file)
raw_notes.head()

def midi_to_notes(midi_file: str) -> pd.DataFrame:
    # Load the MIDI file
    pm = pretty_midi.PrettyMIDI(midi_file)

    # Extract the first instrument (if there are multiple, you can extend to handle all)
    instrument = pm.instruments[0]

    # Get the instrument name (e.g., "Acoustic Grand Piano")
    instrument_name = pretty_midi.program_to_instrument_name(instrument.program)

    # Initialize a dictionary to store the notes
    notes = collections.defaultdict(list)

    # Sort the notes by start time
    sorted_notes = sorted(instrument.notes, key=lambda note: note.start)
    prev_start = sorted_notes[0].start

    for note in sorted_notes:
        start = note.start
        end = note.end
        notes['pitch'].append(note.pitch)  # MIDI pitch (e.g., 60 = Middle C)
        notes['start'].append(start)  # Start time of the note
        notes['end'].append(end)  # End time of the note
        notes['step'].append(start - prev_start)  # Time step from the previous note
        notes['duration'].append(end - start)  # Duration of the note
        notes['velocity'].append(note.velocity)  # Velocity of the note

        prev_start = start

    # Convert numeric pitch to note name for all notes
    get_note_names = np.vectorize(pretty_midi.note_number_to_name)
    note_names = get_note_names(np.array(notes['pitch']))  # Convert pitch to note names

    # Add time signature as 4/4
    time_signature = (4, 4)

    # Convert the dictionary to a DataFrame
    notes_df = pd.DataFrame({name: np.array(value) for name, value in notes.items()})

    # Add additional columns for note names and time signature
    notes_df['note_name'] = note_names  # Add note names
    notes_df['time_signature'] = str(time_signature)  # 4/4 as the fixed time signature
    notes_df['instrument_name'] = instrument_name  # Add instrument name

    return notes_df


def analyze_all_midi_files(directory_path: str):
    # Iterate through all files in the directory
    for file_name in os.listdir(directory_path):
        if file_name.endswith(".mid") or file_name.endswith(".midi"):  # Ensure it is a MIDI file
            midi_file = os.path.join(directory_path, file_name)
            print(f"\nAnalyzing file: {file_name}")

            # Call the midi_to_notes function to process the file
            notes_df = midi_to_notes(midi_file)

            # Print the first few rows of the resulting DataFrame
            print(notes_df.head())  # Show the top rows of the DataFrame

raw_notes2 = midi_to_notes(sample_file)
raw_notes2.head()

"""To visualize the musical piece, plot the note pitch, start and end across the length of the track (i.e. piano roll). Start with the first 100 notes"""

#creating a piano roll visualization
def plot_piano_roll(notes: pd.DataFrame, count: Optional[int] = None):
  if count:
    title = f'First {count} notes'
  else:
    title = f'Whole track'
    count = len(notes['pitch'])
  plt.figure(figsize=(20, 4))
  plot_pitch = np.stack([notes['pitch'], notes['pitch']], axis=0)
  plot_start_stop = np.stack([notes['start'], notes['end']], axis=0)
  plt.plot(
      plot_start_stop[:, :count], plot_pitch[:, :count], color="b", marker=".")
  plt.xlabel('Time [s]')
  plt.ylabel('Pitch')
  _ = plt.title(title)

plot_piano_roll(raw_notes, count=100)

plot_piano_roll(raw_notes)

"""## Creating the training dataset

Creating the training dataset by extracting notes from the MIDI files.
"""

num_files = 5

# Initialize lists to hold the notes
train_notes = []
val_notes = []
test_notes = []

# Process the first `num_files` from the train_files
for f in train_files[:num_files]:
    notes = midi_to_notes(f)
    train_notes.append(notes)

# Concatenate the notes for training
train_notes = pd.concat(train_notes)
train_notes_length = len(train_notes)
print('Number of train notes parsed:', train_notes_length)

# Process the first `num_files` from the val_files
for f in val_files[:num_files]:
    notes = midi_to_notes(f)
    val_notes.append(notes)

# Concatenate the notes for validation
val_notes = pd.concat(val_notes)
val_notes_length = len(val_notes)
print('Number of validation notes parsed:', val_notes_length)

# Process the first `num_files` from the test_files
for f in test_files[:num_files]:
    notes = midi_to_notes(f)
    test_notes.append(notes)

# Concatenate the notes for testing
test_notes = pd.concat(test_notes)
test_notes_length = len(test_notes)
print('Number of test notes parsed:', test_notes_length)

# Define the key order for note features
key_order = ['pitch', 'step', 'duration']

# Convert the notes into numpy arrays
train_notes_array = np.stack([train_notes[key] for key in key_order], axis=1)
val_notes_array = np.stack([val_notes[key] for key in key_order], axis=1)
test_notes_array = np.stack([test_notes[key] for key in key_order], axis=1)

# Create TensorFlow datasets from numpy arrays
train_notes_ds = tf.data.Dataset.from_tensor_slices(train_notes_array)
val_notes_ds = tf.data.Dataset.from_tensor_slices(val_notes_array)
test_notes_ds = tf.data.Dataset.from_tensor_slices(test_notes_array)

# Define sequence creation function
def create_sequences(
    dataset: tf.data.Dataset,
    seq_length: int,
    vocab_size=128,
) -> tf.data.Dataset:
    """Returns TF Dataset of sequence and label examples."""
    seq_length = seq_length + 1

    # Take 1 extra for the labels
    windows = dataset.window(seq_length, shift=1, stride=1, drop_remainder=True)

    flatten = lambda x: x.batch(seq_length, drop_remainder=True)
    sequences = windows.flat_map(flatten)

    # Normalize note pitch
    def scale_pitch(x):
        x = x / [vocab_size, 1.0, 1.0]
        return x

    # Split the labels
    def split_labels(sequences):
        inputs = sequences[:-1]
        labels_dense = sequences[-1]
        labels = {key: labels_dense[i] for i, key in enumerate(key_order)}

        return scale_pitch(inputs), labels

    return sequences.map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)

# Create sequences for train, validation, and test datasets
train_seq_ds = create_sequences(train_notes_ds, seq_length=32)
val_seq_ds = create_sequences(val_notes_ds, seq_length=32)
test_seq_ds = create_sequences(test_notes_ds, seq_length=32)

# Print the output shape of the datasets
print("Train dataset:", train_seq_ds.element_spec)
print("Validation dataset:", val_seq_ds.element_spec)
print("Test dataset:", test_seq_ds.element_spec)

# Check a single example from the sequence dataset for train, validation, and test
print("Train Dataset Example:")
for seq, target in train_seq_ds.take(1):
    print('Sequence Shape:', seq.shape)  # Inspect the shape of the sequence
    print('First 10 Sequence Elements:', seq[0:10])  # Print the first 10 sequence elements (of pitch, step, and duration)
    print()
    print('Target:', target)  # Inspect the target (labels)

print("\nValidation Dataset Example:")
for seq, target in val_seq_ds.take(1):
    print('Sequence Shape:', seq.shape)  # Inspect the shape of the sequence
    print('First 10 Sequence Elements:', seq[0:10])  # Print the first 10 sequence elements (of pitch, step, and duration)
    print()
    print('Target:', target)  # Inspect the target (labels)

print("\nTest Dataset Example:")
for seq, target in test_seq_ds.take(1):
    print('Sequence Shape:', seq.shape)  # Inspect the shape of the sequence
    print('First 10 Sequence Elements:', seq[0:10])  # Print the first 10 sequence elements (of pitch, step, and duration)
    print()
    print('Target:', target)  # Inspect the target (labels)

# Define batch size and buffer size
batch_size = 64
buffer_size = len(train_files) - 32  # The number of items in the dataset minus sequence length (for shuffling)

# Apply batching, shuffling, and prefetching for training dataset
train_ds = (train_seq_ds
            .shuffle(buffer_size)  # Shuffle the training data
            .batch(batch_size, drop_remainder=True)  # Batch the data
            .repeat()  # Repeat the dataset for continuous training
            .cache()  # Cache the dataset for efficient access
            .prefetch(tf.data.experimental.AUTOTUNE))  # Prefetch for faster training

# Check the `element_spec` of the dataset to understand its structure
print("Training dataset element_spec:", train_ds.element_spec)

# Apply the same processing steps to the validation and test datasets
val_ds = (val_seq_ds
          .batch(batch_size, drop_remainder=True)  # Batch validation data
          .cache()  # Cache the dataset
          .prefetch(tf.data.experimental.AUTOTUNE))  # Prefetch for faster validation

test_ds = (test_seq_ds
           .batch(batch_size, drop_remainder=True)  # Batch test data
           .cache()  # Cache the dataset
           .prefetch(tf.data.experimental.AUTOTUNE))  # Prefetch for faster testing

# Print element_spec for validation and test datasets to verify their structure
print("Validation dataset element_spec:", val_ds.element_spec)
print("Test dataset element_spec:", test_ds.element_spec)

"""###Create and train the model"""

def mse_with_positive_pressure(y_true: tf.Tensor, y_pred: tf.Tensor):
  mse = (y_true - y_pred) ** 2
  positive_pressure = 10 * tf.maximum(-y_pred, 0.0)
  return tf.reduce_mean(mse + positive_pressure)

# Input shape and model architecture
seq_length = 32
input_shape = (seq_length, 3)
inputs = tf.keras.Input(input_shape)
x = tf.keras.layers.LSTM(128)(inputs)

outputs = {
    'pitch': tf.keras.layers.Dense(128, name='pitch')(x),
    'step': tf.keras.layers.Dense(1, name='step')(x),
    'duration': tf.keras.layers.Dense(1, name='duration')(x),
}

model = tf.keras.Model(inputs, outputs)

# Loss functions
loss = {
    'pitch': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    'step': mse_with_positive_pressure,
    'duration': mse_with_positive_pressure,
}

# Optimizer and learning rate
learning_rate = 0.005
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

# Compile the model
model.compile(loss=loss, optimizer=optimizer)

model.summary()

# Total samples in each dataset (train, validation, test)
total_samples_train = len(train_files)
total_samples_val = len(val_files)
total_samples_test = len(test_files)

# Print total samples processed
print("Total samples processed (train):", total_samples_train)
print("Total samples processed (validation):", total_samples_val)
print("Total samples processed (test):", total_samples_test)

print(f"Batch size: {batch_size}")

# Calculate steps per epoch for train, validation, and test datasets
train_steps = total_samples_train // batch_size  # Number of batches in one epoch for train dataset
val_steps = total_samples_val // batch_size  # Number of batches in one epoch for validation dataset
test_steps = total_samples_test // batch_size  # Number of batches in one epoch for test dataset

# Print the steps per epoch
print(f"Steps per epoch for training: {train_steps}")
print(f"Steps per epoch for validation: {val_steps}")
print(f"Steps per epoch for testing: {test_steps}")

# Evaluate the model on the training dataset and print results
losses = model.evaluate(train_ds, steps=train_steps, return_dict=True)
print("Initial training losses:", losses)

model.compile(
    loss=loss,
    loss_weights={
        'pitch': 0.05,
        'step': 1.0,
        'duration': 1.0,
    },
    optimizer=optimizer,
)

callbacks = [
    tf.keras.callbacks.ModelCheckpoint(
        filepath='./training_checkpoints/ckpt_{epoch}.weights.h5',
        save_weights_only=True
    ),
    tf.keras.callbacks.EarlyStopping(
        monitor='loss',
        patience=5,
        verbose=1,
        restore_best_weights=True
    ),
]

epochs = 50
steps_per_epoch = len(train_files) // batch_size
history = model.fit(
    train_ds,
    epochs=epochs,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_ds,
    callbacks=callbacks,
)

# Plot training loss
plt.plot(history.epoch, history.history['loss'], label='total loss')
plt.plot(history.epoch, history.history['val_loss'], label='validation loss')  # Plot validation loss
plt.legend()
plt.show()

# Evaluate the model on the test dataset after training is complete
test_losses = model.evaluate(test_ds, return_dict=True)
print("Test dataset evaluation losses:", test_losses)

# Plot the total loss during training
plt.figure(figsize=(10, 6))
plt.plot(history.epoch, history.history['loss'], label='Total Loss', color='b', marker='o')

# Adding titles and labels
plt.title('Learning Curve - Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')

# Displaying the grid for easier interpretation
plt.grid(True)

# Adding legend
plt.legend()

# Show the plot
plt.show()

"""###Generate notes"""

def predict_next_note(
    notes: np.ndarray,
    model: tf.keras.Model,
    temperature: float = 1.0) -> tuple[int, float, float]:
  """Generates a note as a tuple of (pitch, step, duration), using a trained sequence model."""

  assert temperature > 0

  # Add batch dimension
  inputs = tf.expand_dims(notes, 0)

  predictions = model.predict(inputs)
  pitch_logits = predictions['pitch']
  step = predictions['step']
  duration = predictions['duration']

  pitch_logits /= temperature
  pitch = tf.random.categorical(pitch_logits, num_samples=1)
  pitch = tf.squeeze(pitch, axis=-1)
  duration = tf.squeeze(duration, axis=-1)
  step = tf.squeeze(step, axis=-1)

  # `step` and `duration` values should be non-negative
  step = tf.maximum(0, step)
  duration = tf.maximum(0, duration)

  return int(pitch), float(step), float(duration)

import numpy as np
temperature = 2.0
num_predictions = 120

# Define vocab_size here
vocab_size = 128

sample_notes = np.stack([raw_notes[key] for key in key_order], axis=1)

# The initial sequence of notes; pitch is normalized similar to training
# sequences
input_notes = (
    sample_notes[:seq_length] / np.array([vocab_size, 1, 1]))

generated_notes = []
prev_start = 0
for _ in range(num_predictions):
  pitch, step, duration = predict_next_note(input_notes, model, temperature)
  start = prev_start + step
  end = start + duration
  input_note = (pitch, step, duration)
  generated_notes.append((*input_note, start, end))
  input_notes = np.delete(input_notes, 0, axis=0)
  input_notes = np.append(input_notes, np.expand_dims(input_note, 0), axis=0)
  prev_start = start

generated_notes = pd.DataFrame(
    generated_notes, columns=(*key_order, 'start', 'end'))

generated_notes.head(10)

out_file = 'output.mid'
out_pm = notes_to_midi(
    generated_notes, out_file=out_file, instrument_name=instrument_name)
display_audio(out_pm)

from google.colab import files
files.download(out_file)

plot_piano_roll(generated_notes)

def plot_distributions(notes: pd.DataFrame, drop_percentile=2.5):
  plt.figure(figsize=[15, 5])
  plt.subplot(1, 3, 1)
  sns.histplot(notes, x="pitch", bins=20)

  plt.subplot(1, 3, 2)
  max_step = np.percentile(notes['step'], 100 - drop_percentile)
  sns.histplot(notes, x="step", bins=np.linspace(0, max_step, 21))

  plt.subplot(1, 3, 3)
  max_duration = np.percentile(notes['duration'], 100 - drop_percentile)
  sns.histplot(notes, x="duration", bins=np.linspace(0, max_duration, 21))

plot_distributions(generated_notes)

"""#F1 Score Calculation"""

from sklearn.metrics import f1_score
import numpy as np

def discretize_pitch(pitch, bins=12):
    """ Discretize pitch into bins (e.g., 12 for chromatic scale). """
    return int(pitch // (128 / bins))  # Assuming pitch is between 0 and 127 for MIDI

def discretize_step(step, threshold=0.1):
    """ Discretize step values (time between notes) to a binary classification. """
    return 1 if step > threshold else 0  # 1 for step > threshold, else 0

def discretize_duration(duration, threshold=0.1):
    """ Discretize duration values to a binary classification. """
    return 1 if duration > threshold else 0  # 1 for duration > threshold, else 0

# Example data: true values and generated values
true_pitches = [38, 41, 45, 50, 62]
generated_pitches = [65, 72, 64, 43, 60]

true_steps = [0.000000, 0.041667, 0.048177, 0.042969, 0.014323]
generated_steps = [0.154703, 0.015978, 1.025881, 1.115175, 1.224395]

true_durations = [0.194010, 0.184896, 0.148438, 0.122396, 0.113281]
generated_durations = [0.234264, 0.850955, 0.945422, 0.93029, 0.896999]

# Discretize the pitches, steps, and durations into bins or binary values
true_pitches_discretized = [discretize_pitch(p) for p in true_pitches]
generated_pitches_discretized = [discretize_pitch(p) for p in generated_pitches]

true_steps_discretized = [discretize_step(s) for s in true_steps]
generated_steps_discretized = [discretize_step(s) for s in generated_steps]

true_durations_discretized = [discretize_duration(d) for d in true_durations]
generated_durations_discretized = [discretize_duration(d) for d in generated_durations]

# Calculate F1 scores for pitch, step, and duration classification
f1_pitch = f1_score(true_pitches_discretized, generated_pitches_discretized, average='weighted')
f1_step = f1_score(true_steps_discretized, generated_steps_discretized, average='weighted')
f1_duration = f1_score(true_durations_discretized, generated_durations_discretized, average='weighted')

# Print the F1 scores
print(f"F1 Score for pitch: {f1_pitch}")
print(f"F1 Score for step: {f1_step}")
print(f"F1 Score for duration: {f1_duration}")

import pandas as pd
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Example: True and predicted data for pitch, step, and duration (continuous values)
true_data = {
    'pitch': [38, 41, 45, 50, 62],
    'step': [0.000000, 0.041667, 0.048177, 0.042969, 0.014323],
    'duration': [0.194010, 0.184896, 0.148438, 0.122396, 0.113281]
}

generated_data = {
    'pitch': [65, 72, 64, 43, 60],
    'step': [0.154703, 0.015978, 1.025881, 1.115175, 1.224395],
    'duration': [0.234264, 0.850955, 0.945422, 0.93029, 0.896999]
}

# Function to discretize continuous pitch values (adjusted)
def discretize_pitch(pitch, min_pitch=38, max_pitch=88, bins=6):
    bin_width = (max_pitch - min_pitch) / bins
    if pitch < min_pitch:
        return 0  # Assign to the first bin
    if pitch > max_pitch:
        return bins - 1  # Assign to the last bin
    return int((pitch - min_pitch) // bin_width)

# Function to discretize step size (binary classification)
def discretize_step(step, threshold=0.1):
    return 1 if step > threshold else 0  # Binary classification for step size

# Function to discretize duration (binary classification)
def discretize_duration(duration, threshold=0.2):
    return 1 if duration > threshold else 0  # Binary classification for duration

# Discretize the data for pitch, step, and duration
true_pitches = [discretize_pitch(p) for p in true_data['pitch']]
generated_pitches = [discretize_pitch(p) for p in generated_data['pitch']]

true_steps = [discretize_step(s) for s in true_data['step']]
generated_steps = [discretize_step(s) for s in generated_data['step']]

true_durations = [discretize_duration(d) for d in true_data['duration']]
generated_durations = [discretize_duration(d) for d in generated_data['duration']]

# Compute confusion matrices
cm_pitch = confusion_matrix(true_pitches, generated_pitches)
cm_step = confusion_matrix(true_steps, generated_steps)
cm_duration = confusion_matrix(true_durations, generated_durations)

# Plot the confusion matrix for pitch
plt.figure(figsize=(6, 5))
sns.heatmap(cm_pitch, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=[f'Bin {i}' for i in range(cm_pitch.shape[1])],
            yticklabels=[f'Bin {i}' for i in range(cm_pitch.shape[0])])
plt.title('Pitch Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Plot the confusion matrix for step
plt.figure(figsize=(6, 5))
sns.heatmap(cm_step, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Low Step', 'High Step'],
            yticklabels=['Low Step', 'High Step'])
plt.title('Step Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Plot the confusion matrix for duration
plt.figure(figsize=(6, 5))
sns.heatmap(cm_duration, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Short', 'Long'],
            yticklabels=['Short', 'Long'])
plt.title('Duration Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose
from sklearn.model_selection import train_test_split

# Autoencoder class definition
class Autoencoder:
    def __init__(self, input_shape, conv_filters, conv_kernels, conv_strides, latent_space_dim):
        self.input_shape = input_shape
        self.conv_filters = conv_filters
        self.conv_kernels = conv_kernels
        self.conv_strides = conv_strides
        self.latent_space_dim = latent_space_dim
        self.model = None
        self.encoder = None
        self.decoder = None
        self._build()

    def _build(self):
        # Encoder
        encoder_input = Input(shape=self.input_shape, name="encoder_input")
        x = encoder_input
        for filters, kernel, stride in zip(self.conv_filters, self.conv_kernels, self.conv_strides):
            x = Conv2D(filters=filters, kernel_size=kernel, strides=stride, padding="same", activation="relu")(x)
        shape_before_flattening = x.shape[1:]
        x = Flatten()(x)
        latent_space = Dense(self.latent_space_dim, name="latent_space")(x)
        self.encoder = Model(encoder_input, latent_space, name="encoder")

        # Decoder
        decoder_input = Input(shape=(self.latent_space_dim,), name="decoder_input")
        x = Dense(np.prod(shape_before_flattening))(decoder_input)
        x = Reshape(shape_before_flattening)(x)
        for filters, kernel, stride in zip(reversed(self.conv_filters), reversed(self.conv_kernels), reversed(self.conv_strides)):
            x = Conv2DTranspose(filters=filters, kernel_size=kernel, strides=stride, padding="same", activation="relu")(x)
        decoder_output = Conv2DTranspose(filters=1, kernel_size=3, activation="sigmoid", padding="same", name="decoder_output")(x)
        self.decoder = Model(decoder_input, decoder_output, name="decoder")

        # Autoencoder
        autoencoder_input = encoder_input
        autoencoder_output = self.decoder(self.encoder(autoencoder_input))
        self.model = Model(autoencoder_input, autoencoder_output, name="autoencoder")

    def summary(self):
        self.encoder.summary()
        self.decoder.summary()
        self.model.summary()

    def compile(self, learning_rate):
        self.model.compile(optimizer=Adam(learning_rate=learning_rate), loss="mse")

    def train(self, x_train, x_valid, batch_size, epochs):
        history = self.model.fit(
            x_train, x_train,
            validation_data=(x_valid, x_valid),
            batch_size=batch_size,
            epochs=epochs,
            shuffle=True
        )
        return history

# Constants
LEARNING_RATE = 0.0005
BATCH_SIZE = 32
EPOCHS = 20

# Function to load and preprocess MIDI dataset
def load_midi_dataset(filenames):
    """Loads and preprocesses the MIDI dataset."""
    print('Number of files:', len(filenames))
    dataset = []

    for file in filenames:
        # Replace this placeholder with actual MIDI-to-matrix conversion logic
        midi_data = np.random.rand(128, 128)  # Example placeholder for MIDI processing
        dataset.append(midi_data)

    dataset = np.array(dataset)
    dataset = dataset.astype("float32") / np.max(dataset)  # Normalize
    dataset = np.expand_dims(dataset, axis=-1)  # Add channel dimension

    return dataset

# Function to train the autoencoder
def train_autoencoder_model(x_train, x_valid, learning_rate, batch_size, epochs):
    """Initializes, compiles, and trains the Autoencoder."""
    autoencoder = Autoencoder(
        input_shape=(128, 128, 1),  # Update based on your MIDI encoding shape
        conv_filters=[32, 64, 64, 64],
        conv_kernels=[3, 3, 3, 3],
        conv_strides=[1, 2, 2, 1],
        latent_space_dim=16  # Adjust as needed
    )
    autoencoder.summary()
    autoencoder.compile(learning_rate)
    history = autoencoder.train(x_train, x_valid, batch_size, epochs)
    return autoencoder, history

# Function to plot training loss graph
def plot_training_loss(history):
    """Plots the training loss graph."""
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss Over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid()
    plt.show()

# List of filenames (provide this as input to the script)
# Example: filenames = ["file1.mid", "file2.mid", ...]
filenames = ["example1.mid", "example2.mid", "example3.mid"]

# Load and preprocess the dataset
x_data = load_midi_dataset(filenames)

# Split the dataset into train, validation, and test sets
x_train, x_temp = train_test_split(x_data, test_size=0.3, random_state=42)
x_valid, x_test = train_test_split(x_temp, test_size=0.5, random_state=42)

# Train the autoencoder
autoencoder, history = train_autoencoder_model(x_train, x_valid, LEARNING_RATE, BATCH_SIZE, EPOCHS)

# Plot training and validation loss
plot_training_loss(history)

# Optionally, evaluate the model on the test set
test_loss = autoencoder.model.evaluate(x_test, x_test)
print("Test Loss:", test_loss)



"""##Implement double stacked lstm"""

import tensorflow as tf

# Custom loss function with positive pressure
def mse_with_positive_pressure(y_true: tf.Tensor, y_pred: tf.Tensor):
    mse = (y_true - y_pred) ** 2
    positive_pressure = 10 * tf.maximum(-y_pred, 0.0)
    return tf.reduce_mean(mse + positive_pressure)

# Input shape and model architecture
seq_length = 32
input_shape = (seq_length, 3)
inputs = tf.keras.Input(shape=input_shape)

# First LSTM layer with return_sequences=True
x = tf.keras.layers.LSTM(128, return_sequences=True)(inputs)

# Second LSTM layer
x = tf.keras.layers.LSTM(64)(x)  # No need to set return_sequences=False as it's default

# Output layers for pitch, step, and duration
outputs = {
    'pitch': tf.keras.layers.Dense(128, name='pitch')(x),
    'step': tf.keras.layers.Dense(1, name='step')(x),
    'duration': tf.keras.layers.Dense(1, name='duration')(x),
}

# Define the model
model = tf.keras.Model(inputs, outputs)

# Loss functions
loss = {
    'pitch': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    'step': mse_with_positive_pressure,
    'duration': mse_with_positive_pressure,
}

# Optimizer and learning rate
learning_rate = 0.005
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

# Compile the model
model.compile(loss=loss, optimizer=optimizer)

# Model summary
model.summary()

# Total samples in each dataset (train, validation, test)
total_samples_train = len(train_files)
total_samples_val = len(val_files)
total_samples_test = len(test_files)

# Print total samples processed
print("Total samples processed (train):", total_samples_train)
print("Total samples processed (validation):", total_samples_val)
print("Total samples processed (test):", total_samples_test)

# Calculate steps per epoch for train, validation, and test datasets
train_steps = total_samples_train // batch_size  # Number of batches in one epoch for train dataset
val_steps = total_samples_val // batch_size  # Number of batches in one epoch for validation dataset
test_steps = total_samples_test // batch_size  # Number of batches in one epoch for test dataset

# Print the steps per epoch
print(f"Steps per epoch for training: {train_steps}")
print(f"Steps per epoch for validation: {val_steps}")
print(f"Steps per epoch for testing: {test_steps}")

# Evaluate the model on the training dataset and print results
losses = model.evaluate(train_ds, steps=train_steps, return_dict=True)
print("Initial training losses:", losses)

model.compile(
    loss=loss,
    loss_weights={
        'pitch': 0.05,
        'step': 1.0,
        'duration': 1.0,
    },
    optimizer=optimizer,
)

callbacks = [
    tf.keras.callbacks.ModelCheckpoint(
        filepath='./training_checkpoints/ckpt_{epoch}.weights.h5',
        save_weights_only=True
    ),
    tf.keras.callbacks.EarlyStopping(
        monitor='loss',
        patience=5,
        verbose=1,
        restore_best_weights=True
    ),
]

epochs = 50
steps_per_epoch = len(train_files) // batch_size
history = model.fit(
    train_ds,
    epochs=epochs,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_ds,
    callbacks=callbacks,
)

